---
title: "Final_exam"
author: "Gideon Popoola"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---
## Academic Honesty Statement

I,Gideon Popoola, hereby state that I have not communicated with or gained information in any way from my classmates or anyone other than the course instructor during this exam, and that all work is my own.

## Synthetic data question

## Question 1

```{r}
library(rstanarm)
library(tidyverse)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
n <- 5000
x <- seq(.01, 50, length.out = n)
beta <- c(-5, .25)
p1 <- invlogit(beta[1] + beta[2] * x)
log_odds <- logit(p1)
y1 <- rbinom(n,1,p1)
combined <- tibble(x=x, y=y1)
combined %>%
 ggplot(aes(y=y1, x=x)) + 
  geom_smooth(formula = 'y~x', method ='loess') + 
  geom_smooth(formula = 'y~x', method = 'lm', color = 'red') + 
  geom_point() + ylab("log-odds")+
  theme_bw() +
  ggtitle('Relationship between log-odd and x for first model') +
  labs(caption = "Red is best linear fit, blue is loess curve ")
```

```{r}
n <- 5000
x <- seq(.01, 50, length.out = n)
x1<- log(x)
beta <- c(-10, 4)
p1 <- invlogit(beta[1] + beta[2] * x1)
log_odds <- logit(p1)
y1 <- rbinom(n,1,p1)
combined1 <- tibble(x=x, y=y1)
combined1 %>%
 ggplot(aes(y=y1, x=x)) + 
  geom_smooth(formula = 'y~x', method ='loess') + 
  geom_smooth(formula = 'y~x', method = 'lm', color = 'red') + 
  geom_point() + ylab("log-odds")+
  theme_bw() +
  ggtitle('Relationship between log-odds and x for second model') +
  labs(caption = "Red is best linear fit, blue is loess curve ")

```

```{r}
n <- 5000
x <- seq(.01, 50, length.out = n)
x_new <- (x)^2
beta <- c(-5, .5, -.01)
p1 <- invlogit(beta[1] + beta[2] * x + beta[3]*x_new)
log_odds <- logit(p1)
y1 <- rbinom(n,1,p1)
combined2 <- tibble(x=x, y=y1)
combined2 %>%
 ggplot(aes(y=y1, x=x)) + 
  geom_smooth(formula = 'y~x', method ='loess') + 
  geom_smooth(formula = 'y~x', method = 'lm', color = 'red') + 
  geom_point() + ylab("log-odds")+
  theme_bw() +
  ggtitle('Relationship between log-odd and x for third model') +
  labs(caption = "Red is best linear fit, blue is loess curve ")


```

## Question 2
I couldn't figure out how to plot the true model curve using the beta parameters and link function. I also tried to check our class note, but I couldn't find it anywhere in the class note.

```{r}
#first model
simulation1 <- glm(y~x, data = combined, family = binomial(link = "logit"))
combined %>% ggplot(aes(y = y, x =x )) + geom_smooth(method ='loess', formula = 'y~x', color = 'red', se =F) + #LOESS curve
 geom_abline(intercept = -5, slope = .25, color = 'green')+#true model curve
  geom_point(alpha = .1)+ labs(caption = "Red is loess curve, blue is log model curve, and green is true model curve ")+
  geom_rug() + ggtitle('Plot of the first model ') +  theme_bw() + xlab('X') + ylab("y")+
  geom_line(inherit.aes = F, data = tibble(Heart= seq(-50, 50, by = .1), 
            y = plogis(coef(simulation1)['(Intercept)'] + coef(simulation1)['x']*Heart)),
             aes(x=Heart, y=y), color = 'blue',lwd = 1)# curve from logistic model


#second model
simulation2 <- glm(y~x, data = combined1, family = binomial) # Given model
combined1 %>% ggplot(aes(y = y, x =x )) + geom_abline(intercept = -10, slope = 4, color = 'green')+
  geom_point(alpha = .1) + geom_smooth(method ='loess', formula = 'y~x', color = 'red', se =F)+labs(caption = "Red is loess curve, blue is log model curve, and green is true model curve ")+
  geom_rug() + ggtitle('Plot of the second model ') + 
  theme_bw() + xlab('X') + ylab("y")+
geom_line(inherit.aes = F, data = tibble(Heart= seq(-50, 50, by = .1), 
            y = plogis(coef(simulation2)['(Intercept)'] + coef(simulation2)['x']*Heart)),
             aes(x=Heart, y=y), color = 'blue',lwd = 1)

#third model
simulation3 <- glm(y~x, data = combined2, family = binomial)
combined2 %>% ggplot(aes(y = y, x =x )) + 
  geom_point(alpha = .1) + geom_smooth(method ='loess', formula = 'y~x', color = 'red', se =F)+ #loess curve
geom_abline(intercept = -5, slope = .5, color = 'green')+ #true model curve
  geom_rug() + ggtitle('Plot of the third model') + 
  theme_bw() + xlab('X') +ylab("y")+labs(caption = "Red is loess curve, blue is log model curve, and green is true model curve ")+
  geom_line(inherit.aes = F, data = tibble(Heart= seq(-50, 50, by = .1), 
            y = plogis(coef(simulation3)['(Intercept)'] + coef(simulation3)['x']*Heart)),
             aes(x=Heart, y=y), color = 'blue',lwd = 1)#logistic curve


```

## Question 3
I didn't see any shape after fitting the given model above and creating a residual diagnostic report from simulation 1 (the first model), which matches my expectation because we didn't violate the functional form. From simulation 2 (the second model), I can see a slight any shape, and this match expectation as we violated the functional form. From Simulation 3 (the third model), I can see shape in the residual, and this matches my expectation since we violate the functional form. Overall, we are not supposed to see shape in the residual of a model, but if we violate the functional form of the model, the model will not be able to capture the relationship between the data, and this will lead to a bad model that gives shape to the residual.


```{r cars}
library(ggResidpanel)
suppressWarnings(library(arm))
simulation1 <- glm(y~x, data = combined, family = binomial(link = "logit"))
binnedplot(predict(simulation1, type = 'response'), resid(simulation1), xlab = "Estimated probability") 


simulation2 <- glm(y~ x, data = combined1, family = binomial(link = "logit")) 
binnedplot(predict(simulation2, type = 'response'), resid(simulation2), xlab = "Estimated probability")

simulation3 <- glm(y~x, data = combined2, family = binomial) 
binnedplot(predict(simulation3, type = 'response'), resid(simulation3), xlab = "Estimated probability")

```


## Question 4
After creating the correctly specified model, upon checking the residual, I didn't see any shape, and this matched my expectation because we'd taken care of the violated functional form. In other words, we are not violating the functional form again.

```{r}
simulation1 <- glm(y~x, data = combined, family = binomial(link = "logit"))
binnedplot(predict(simulation1, type = 'response'), resid(simulation1), xlab = "Estimated probability")


simulation2 <- glm(y ~ x1, data = combined1, family = binomial(link = "logit")) #x1 is log x
binnedplot(predict(simulation2, type = 'response'), resid(simulation2), xlab = "Estimated probability")


simulation3 <- glm(y~x + x_new, data = combined2, family = binomial(link =  "logit")) #x_new is x_square
binnedplot(predict(simulation3, type = 'response'), resid(simulation3), xlab = "Estimated probability")


```

## Question 5
Violating the functional form of a logistic regression model can lead to a poorly fitted model. The resulting implication of violating this functional form can range from mild to severe. This can be verified by comparing models 2 and 3, and we can conclude that the impact of this violation was greater on model 3 than on model 2. What worries me about using the incorrectly specified model in question 3 is that the model will be unable to capture the relationship between the predictors and the data, resulting in an incorrect model or miss prediction. This kind of problem can void a whole experiment.


## Modelling Question(part 1)

## A

```{r pressure, echo=TRUE, fig.cap= 'The figure above shows the box plot of houses and their respective zip codes. The zipcode variable has been centered.'}
KingCo1 <- read_csv("https://raw.githubusercontent.com/STAT505/FinalExam/main/KingCo_wPrice.csv", show_col_types = FALSE) %>% mutate(zipcode = zipcode - mean(zipcode))
ggplot(KingCo1, aes(x=zipcode, y= price, group = as.factor(zipcode))) + 
        geom_boxplot()+xlab("Zipcode")+ylab("Price ($)")+ggtitle("Box plot of house price vs zipcode")

```




```{r}
model1 = lm(price ~ zipcode, data = KingCo1)
summary(model1)

```
## model summary (B) 

I centered my zipcode variable.

The following model's summary

The intercept is the price of the house when the zipcode is 0. The price of the house is 632591 with an uncertain interval of 21426. This uncertainty is the difference between our estimated price and the actual price. The uncertainty is big, but since there are many factors that contribute to a house's price apart from being in the same zipcode, this is bound to happen.

Zipcode: For every one unit changed in the zipcode, the house price increases by 1548, with an uncertainty of 494.4. This uncertainty shows the difference between our estimated and actual values.

Looking at this prediction, we can see the difference in price range as we move from one zip code to the next. This difference can be attributed to the amenities attached to each zipcode area as we move from one zipcode to another. A good example of this is between Belgrade and Bozeman, where the same house with the same sqft_living, sqft_lot, bathrooms, and bedrooms might cost more in Bozeman than in Belgrade because of amenities such as easy access to grocery stores (Walmart and Winco), easy access to public transportation (Streamline), and other amenities like that. This example shows how zipcode can affect the price of a house as we move from one zipcode area to the next.

```{r}
library(ggResidpanel)
#resid_panel(model1, plots = 'all', smoother = T, qqbands = T)
#resid_interact(model1, plots = c("resid", "qq")) # note won't compile in non HTML formats
resid_xpanel(model1)
```



## Model Assumptions (C)

According to our class discussion, there are six assumptions that can affect the regression model. The first two are attached to the dataset; hence, I will not be discussing them here. The other four assumptions are discussed in relation to my model above;


Additivity and Linearity: Because there is no interaction in the model and no predictor is transformed, the additivity and linearity assumptions are not violated. However, I am concerned about this because it might violate the functional relationship in the dataset and lead to bad predictions.

Independence of error: Since some houses belong to the same zip code, this model violates the independence assumption. I'm not really concerned about this because houses will surely belong to the same zipcode, and we can't do much about it.

Equal variance of error: This model did not violate the equal variance of error assumption. According to the ROS textbook, this assumption doesn't have a big impact on the regression model, so I'm not concerned about it.

Normality of error: This model violates the normality of error assumption. This can be verified from the Q-Q plot of the model. According to the ROS textbook, this assumption doesn't have a big impact on the regression model, so I'm not concerned about it.

## Model specification(Part 2)

$$y = \beta_0 + \beta_1x_{=zipcode}+\beta_2x_{=sqft-living} + \beta_3x_{=sqft-lot}+ \beta_4x_{=bathrooms}*x_{=bedrooms} + \epsilon  $$

$$\epsilon \sim N(0, \sigma^2)$$
$y$ is the price of the the house
$\beta_0$ is the price of the house when all the predictor are zero

$\beta_1 x_{=zipcode}$ is the expected increase or decrease in house price when zipcode changes by 1 units and all other predictor are constant.

$\beta_2 x_{=sqft-living}$ is the expected increase or decrease in house price when square foot of living space changes by 1 foot and all other predictor are constant

$\beta_2 x_{=sqft-lot}$ is the expected increase or decrease in house price when square foot of lot space changes by 1 foot and all other predictor are constant

$\beta_3 x_{=bathrooms} x_{=bedroom}$ is the interaction term which can be interpreted as the difference in house price for a house with both changes in bedrooms and bathrooms. 

$\epsilon$ is random model error.



## Justification of the model selected(Question B) 

I chose the model above because of the following reasons:

1. Exploratory data analysis: I can deduce from my EDA that I some predictors had a distinct samples that clearly separates the two classes. These predictors tend to make good predictors; hence, I added them to my model.

2. Result: While simulating the model, I keep an eye on predictors that yield a good p-value and increase the adjusted r-square. Any predictor that gives improvement in any of the areas is added to my model.

3. Human intuition: The interaction between bedrooms and bathrooms is my human intuition based on my knowledge of real estate, and it turns on to improve the performance of the model.

```{r}
model2 = lm(price ~zipcode + sqft_living + sqft_lot + bathrooms*bedrooms, data = KingCo1)


```

## Question C
```{r}
newdata = data.frame(bedrooms = 3, bathrooms = 2, sqft_living = 2000, sqft_lot = 70000, waterfront = 1, zipcode = 98070)
newdata1 = data.frame (bedrooms = 3, bathrooms =2, sqft_living = 2000, sqft_lot = 70000, waterfront = 1, zipcode = 98032)
newdata3 = data.frame(bedrooms = 1, bathrooms =1.5, sqft_living = 1000, sqft_lot = 6000, waterfront = 0, zipcode = 98102)
newdata4 = data.frame(bedrooms = 4, bathrooms =2.5, sqft_living = 2600, sqft_lot = 50000, waterfront = 0, zipcode = 98014)


predictmodel <- predict(model2, newdata = newdata, interval = "prediction")
predictmodel1<- predict(model2, newdata =newdata1, interval = "prediction")
predictmodel2<- predict(model2, newdata = newdata3, interval = "prediction")
predictmodel3<- predict(model2, newdata = newdata4, interval = "prediction")
print(predictmodel)
print(predictmodel1)
print(predictmodel2)
print(predictmodel3)

```
